{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec3. LangChain\n",
    "\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by language models. It provides abundant abstractions about langage models and sources of context (prompt instructions, few shot examples, content to ground its response in, etc.), which enable the user to easily **chain** these components together for developing awesome applications.\n",
    "\n",
    "In this lab, we will learn several key abstractions in LangChain and build an input-output customized AI-powered web-search application.\n",
    "\n",
    "### Reference \n",
    "1. [Langchain document](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "\n",
    "## 0. First thing first\n",
    "\n",
    "### 0.1 Dependencies and Keys\n",
    "  \n",
    "In addition to the Open AI keys, add the following keys to your .env file.\n",
    "\n",
    "- Serp api key:\n",
    "    ```\n",
    "    SERPAPI_API_KEY=\"YOURKEY\"\n",
    "    ```\n",
    "    The `SERPAPI_API_KEY` is for invoking the search engine, first register through this [web site](https://serpapi.com/).\n",
    "\n",
    "    After getting these two keys, set your keys as environment variables.\n",
    "- Langchain API key (for tracing)\n",
    "    ```\n",
    "    LANGCHAIN_TRACING_V2=\"true\"\n",
    "    LANGCHAIN_API_KEY=ls_xxxxxxxx\n",
    "    ```\n",
    "    To create a `LANGCHAIN_API_KEY`, you can register through the [LANGSMITH](https://docs.smith.langchain.com/)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have installed these dependencies in your image\n",
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv  \n",
    "import os  \n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL=\"qwen2.5-72b-instruct\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.environ.get(\"INFINI_API_KEY\")  # langchain use this environment variable to find the OpenAI API key\n",
    "OPENAI_BASE=os.environ.get(\"INFINI_BASE_URL\") # will be used to pass the OpenAI base URL to langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key abstractions in LangChain\n",
    "\n",
    "| Abstracted Components | Input Type                                | Output Type           |\n",
    "|-----------------------|-------------------------------------------|-----------------------|\n",
    "| Prompt                | Dictionary                                | PromptValue           |\n",
    "| ChatModel             | string, list of messages or a PromptValue | string, ChatMessage   |\n",
    "| OutputParser          | The output of an LLM or ChatModel         | Depends on the parser |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The ChatModel\n",
    "\n",
    "`ChatModels` is a language model which takes a list of messages or a string as input and returns a message or a string.\n",
    "\n",
    "`ChatModel` provides two methods to interact with the user:\n",
    "\n",
    "- `predict`: takes in a string, returns a string.\n",
    "- `predict_messages`: takes in a list of messages, returns a message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat_model = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model=CHAT_MODEL,\n",
    "    base_url=OPENAI_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an output utility to show the type and the content of the result\n",
    "def print_with_type(res):\n",
    "    print(f\"%s\" % (type(res)))\n",
    "    print(f\"%s\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four roles in LangChain, and you can define your own custom roles.\n",
    "\n",
    "- `HumanMessage`: A ChatMessage coming from a human/user.\n",
    "- `AIMessage`: A ChatMessage coming from an AI/assistant.\n",
    "- `SystemMessage`: A ChatMessage coming from the system.\n",
    "- `FunctionMessage`: A ChatMessage coming from a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Hello Xu Wei! Nice to meet you too! I\\'d be happy to tell you about LangChain.\\n\\nLangChain is a framework for developing applications that use large language models (LLMs). It provides a set of tools and abstractions to help developers build, deploy, and manage applications that leverage the capabilities of LLMs, such as generating text, summarizing documents, translating languages, and more.\\n\\n### Key Features of LangChain:\\n\\n1. **Modular Architecture**:\\n   - LangChain is designed to be modular, allowing you to mix and match different components to suit your specific needs. This includes different language models, data sources, and output formats.\\n\\n2. **Integration with LLMs**:\\n   - LangChain supports integration with various large language models, including those from different providers like OpenAI, Anthropic, and others. This flexibility allows you to choose the best model for your application.\\n\\n3. **Data Handling**:\\n   - The framework provides tools for handling and processing data, including text preprocessing, data augmentation, and data loading from various sources.\\n\\n4. **Prompt Engineering**:\\n   - LangChain includes utilities for creating and managing prompts, which are crucial for guiding the behavior of LLMs. This can help you fine-tune the output to better match your application\\'s requirements.\\n\\n5. **Chain of Thought**:\\n   - One of the key concepts in LangChain is the \"chain of thought\" approach, where the model is guided through a series of steps to solve complex problems. This can involve breaking down a task into smaller, more manageable parts.\\n\\n6. **Evaluation and Testing**:\\n   - LangChain provides tools for evaluating the performance of your models and applications, including metrics and testing frameworks.\\n\\n7. **Deployment**:\\n   - The framework supports various deployment options, including cloud services, on-premises servers, and containerized environments. This makes it easier to scale and manage your applications.\\n\\n8. **Community and Ecosystem**:\\n   - LangChain has a growing community of developers and users, which means you can find a lot of resources, tutorials, and support. The ecosystem also includes a variety of plugins and extensions that can enhance your application.\\n\\n### Use Cases:\\n\\n- **Chatbots and Virtual Assistants**: Building conversational agents that can understand and respond to user queries.\\n- **Content Generation**: Generating articles, stories, and other types of content.\\n- **Summarization**: Creating summaries of long documents or articles.\\n- **Translation**: Translating text from one language to another.\\n- **Code Generation**: Generating code snippets or entire programs.\\n- **Data Analysis**: Analyzing and interpreting data using natural language.\\n\\n### Getting Started:\\n\\nIf you\\'re interested in getting started with LangChain, you can visit the official documentation and GitHub repository. The documentation provides detailed guides, tutorials, and examples to help you get up and running quickly.\\n\\nI hope this gives you a good overview of LangChain! If you have any more questions or need further information, feel free to ask!' response_metadata={'token_usage': {'completion_tokens': 613, 'prompt_tokens': 50, 'total_tokens': 663}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': []}} id='run-99933cf6-22cf-48b1-b16e-b2cdd9c22df0-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "qtext = \"hello! my name is xu wei, nice to meet you! could you tell me something about langchain\"\n",
    "\n",
    "messages = []\n",
    "messages.append(\n",
    "    HumanMessage(content=qtext)  # construct a human message\n",
    "    )\n",
    "res = chat_model.invoke(messages)  # invoke the chat model\n",
    "\n",
    "print_with_type(res)\n",
    "\n",
    "messages.append(res)  # append the result to the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructors are tedious to use, and you can use the following more friendly API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"I'm just here to help you out! What can I assist you with today?\" response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 50, 'total_tokens': 68}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': []}} id='run-55b6f2ec-6fb2-4356-bb00-00de5900fed6-0'\n"
     ]
    }
   ],
   "source": [
    "# a simpler way to manage messages\n",
    "from langchain.memory import ChatMessageHistory\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")  # avoid using the constructor directly\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.add_user_message(\"nothing much, you?\")\n",
    "\n",
    "res = chat_model.invoke(history.messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangChain has a wide range of applications across various industries and use cases. Here are some of the key areas where LangChain can be applied:\\n\\n### 1. **Chatbots and Virtual Assistants**\\n- **Customer Support**: Building chatbots that can handle customer inquiries, provide product information, and resolve issues.\\n- **Personal Assistants**: Creating virtual assistants that can manage schedules, send reminders, and perform tasks on behalf of users.\\n\\n### 2. **Content Generation**\\n- **Article Writing**: Generating articles, blog posts, and news articles.\\n- **Creative Writing**: Producing stories, poems, and other creative content.\\n- **Marketing Copy**: Creating marketing materials, ad copy, and product descriptions.\\n\\n### 3. **Summarization**\\n- **Document Summarization**: Creating concise summaries of long documents, reports, and articles.\\n- **Meeting Notes**: Generating summaries of meeting transcripts or recordings.\\n\\n### 4. **Translation**\\n- **Text Translation**: Translating text from one language to another.\\n- **Multilingual Content**: Creating content in multiple languages for a global audience.\\n\\n### 5. **Code Generation**\\n- **Programming Assistance**: Generating code snippets, functions, and entire programs.\\n- **Code Documentation**: Creating documentation for codebases and APIs.\\n\\n### 6. **Data Analysis and Interpretation**\\n- **Data Summarization**: Summarizing and interpreting large datasets.\\n- **Report Generation**: Creating detailed reports and visualizations from data.\\n\\n### 7. **Educational Tools**\\n- **Tutoring Systems**: Building systems that can provide personalized tutoring and feedback.\\n- **Content Creation**: Generating educational content, quizzes, and exercises.\\n\\n### 8. **Legal and Compliance**\\n- **Contract Analysis**: Analyzing and summarizing legal documents and contracts.\\n- **Compliance Checking**: Ensuring that documents and processes comply with regulations.\\n\\n### 9. **Healthcare**\\n- **Medical Notes**: Summarizing and analyzing medical records and patient notes.\\n- **Health Information**: Providing health-related information and advice.\\n\\n### 10. **Financial Services**\\n- **Financial Analysis**: Analyzing financial data and generating reports.\\n- **Investment Advice**: Providing investment recommendations and financial planning.\\n\\n### 11. **Customer Insights**\\n- **Sentiment Analysis**: Analyzing customer feedback and reviews to understand sentiment.\\n- **Customer Segmentation**: Segmenting customers based on their behavior and preferences.\\n\\n### 12. **Personalization**\\n- **Product Recommendations**: Generating personalized product recommendations based on user behavior.\\n- **Content Personalization**: Tailoring content to individual users based on their preferences and history.\\n\\n### 13. **Research and Development**\\n- **Literature Review**: Summarizing and analyzing research papers and articles.\\n- **Hypothesis Generation**: Generating hypotheses and research questions.\\n\\n### 14. **Entertainment**\\n- **Game Development**: Creating game narratives, dialogues, and character interactions.\\n- **Script Writing**: Generating scripts for movies, TV shows, and plays.\\n\\n### 15. **Customer Experience**\\n- **Feedback Analysis**: Analyzing customer feedback to improve products and services.\\n- **User Experience**: Enhancing user experience through personalized interactions and content.\\n\\n### Example Use Case: Customer Support Chatbot\\nImagine you are building a customer support chatbot for an e-commerce platform. Using LangChain, you can:\\n- **Integrate with a Language Model**: Use a pre-trained language model to understand and respond to customer queries.\\n- **Handle Complex Queries**: Implement a chain of thought approach to break down complex queries into simpler steps.\\n- **Generate Responses**: Automatically generate responses to common questions.\\n- **Summarize Interactions**: Summarize customer interactions for follow-up and record-keeping.\\n- **Translate Responses**: Provide support in multiple languages by translating responses.\\n\\n### Getting Started\\nTo get started with LangChain, you can:\\n1. **Install LangChain**: Use pip to install the LangChain library.\\n   ```bash\\n   pip install langchain\\n   ```\\n2. **Explore the Documentation**: Visit the official LangChain documentation for detailed guides and tutorials.\\n3. **Try Examples**: Experiment with the provided examples to understand how different components work.\\n4. **Build Your Application**: Start building your application by integrating LangChain with your chosen language model and data sources.\\n\\nI hope this gives you a comprehensive understanding of the applications of LangChain! If you have any specific use cases in mind or need more detailed information, feel free to ask!' response_metadata={'token_usage': {'completion_tokens': 913, 'prompt_tokens': 677, 'total_tokens': 1590}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': []}} id='run-c0326c96-6c7c-4d8c-b0d8-380fc259b2f0-0'\n"
     ]
    }
   ],
   "source": [
    "# remembering the chat history and context\n",
    "\n",
    "qtext = \"what is its application?\"\n",
    "messages.append(HumanMessage(content=qtext))  ## providing context of chat histroy\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n",
    "messages.append(res)  ## remembers the histroy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prompt templates\n",
    "\n",
    "LangChain provides ``PromptTemplate`` to help with formatting the prompts. \n",
    "A ``PromptTemplate`` allows you to define a template with placeholders that can be filled in with specific values at runtime. \n",
    "This helps in creating dynamic and reusable prompts for different contexts and inputs.\n",
    "\n",
    "The most plain prompt is in the type of a ``string``. Usually, the prompt includes several different type of `Messages`, which contains the `role` and the plain prompt as `content`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Simple template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "What is a good name for a company that makes candies?\n"
     ]
    }
   ],
   "source": [
    "# Prompt Template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "input_prompt = prompt.format(product=\"candies\")\n",
    "\n",
    "print_with_type(input_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.'), HumanMessage(content='I love programming.')]\n"
     ]
    }
   ],
   "source": [
    "# Chat Template (a list of temlates in a chat prompt template)\n",
    "\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "# format chat message prompt\n",
    "sys_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\")\n",
    "\n",
    "print_with_type(chat_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Using template in the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.'), HumanMessage(content='what is its application?')]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 29, 'total_tokens': 34}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': []}} id='run-dfd47541-f2b9-4db8-b23b-ddbf1e5b9ba5-0'\n"
     ]
    }
   ],
   "source": [
    "# format messages with PromptTemplate with translator as an example\n",
    "messages = []\n",
    "chat_input = chat_prompt.format_messages(input_language=\"English\", output_language=\"Chinese\", text=qtext)\n",
    "print_with_type(chat_input)\n",
    "print_with_type(chat_model.invoke(chat_input))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.'), HumanMessage(content='what is its application?')]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 29, 'total_tokens': 34}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': []}} id='run-90c4894e-bb7b-4ea4-ab7d-79e85d9a1ee5-0'\n"
     ]
    }
   ],
   "source": [
    "messages = chat_input + messages  ## the system message must be at the beginning\n",
    "print_with_type(messages)\n",
    "\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chaining Components together\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. \n",
    "In LangChain, most of the above key abstraction components are `Runnable` objects, and we can **chain** them together to build awesome applications. \n",
    "\n",
    "LangChain makes the chainning powerful through **LangChain Expression Language (LCEL)**, which can support chainning in manners of:\n",
    "\n",
    "- Async, Batch, and Streaming Support: any chain constructed in LCEL can automatically have full synv, async, batch and streaming support. \n",
    "- Fallbacks: due to many factors like network connection or non-deterministic properties, your LLM applications need to handle errors gracefully. With LCEL, your can easily attach fallbacks any chain.\n",
    "- Parallelism: since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "- LangSmith Tracing Integration: (for debugging, see below).\n",
    "\n",
    "In this lab, we only demonstrate the simplest functional chainning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='{\"这是输入到聊天模型链和聊天提示的内容。\"}' response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 39, 'total_tokens': 53}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': []}} id='run-796a2ae4-0348-4271-ae90-5b5b422361af-0'\n"
     ]
    }
   ],
   "source": [
    "# More abstractions: bundling prompt and the chat_model into a chain\n",
    "\n",
    "translate_chain = chat_prompt | chat_model\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out = translate_chain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Chinese\", \n",
    "    \"text\": {qtext}\n",
    "    })\n",
    "print_with_type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Output parser\n",
    "\n",
    "Language models output text, which is often unstructured and free-form. However, in many applications, you may need more structured information to work with, such as JSON, XML, or other formats. This is where output parsers come in.\n",
    "\n",
    "Output parsers are tools that transform the raw text output of language models into structured data formats. The motivation for using output parsers is to facilitate easier data manipulation, integration, and analysis. By converting text into structured formats, you can more effectively utilize the information in downstream applications, automate workflows, and ensure consistency in data handling.\n",
    "\n",
    "LangChain provides several commonly-used output parsers, including:\n",
    "- [JSONparser](https://python.langchain.com/docs/how_to/output_parser_json/): Converts text output into JSON format.\n",
    "- [XMLparser](https://python.langchain.com/docs/how_to/output_parser_xml/): Converts text output into XML format.\n",
    "- [YAMLparser](https://python.langchain.com/docs/how_to/output_parser_yaml/): Converts text output into YAML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Simple output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{\"这是输入到聊天模型链和聊天提示的内容。\"}\n"
     ]
    }
   ],
   "source": [
    "# a simple output parser\n",
    "# StdOutParser converts the chat message to a string.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "stdoutchain = chat_prompt | chat_model | output_parser\n",
    "\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out=stdoutchain.invoke({\n",
    "    \"input_language\": \"English\", \n",
    "    \"output_language\": \"Chinese\", \n",
    "    \"text\": {qtext}\n",
    "    })\n",
    "print_with_type(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Advanced output parsers: from Results to a Python Object\n",
    "Here we demonstrate the powerful Json Outputparser as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': 'A Survey of Deep Learning System Optimization, Deep Learning-based Text Representation: A Survey, Efficient and Effective Algorithms for Training Quantized Neural Networks, and more'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Professor(BaseModel):\n",
    "    name: str = Field(description=\"name of the Professor\")\n",
    "    publications: str = Field(description=\"the string of the professor's publications, separated by comma\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "professor_chain = prompt | chat_model | parser\n",
    "query = \"tell me about professor Wei Xu including his publications.\"\n",
    "output = professor_chain.invoke({\n",
    "    \"query\": {query}\n",
    "    })\n",
    "print_with_type(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "input_variables=['query'] partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"name of the Professor\", \"title\": \"Name\", \"type\": \"string\"}, \"publications\": {\"description\": \"the string of the professor\\'s publications, separated by comma\", \"title\": \"Publications\", \"type\": \"string\"}}, \"required\": [\"name\", \"publications\"]}\\n```'} template='Answer the user query.\\n{format_instructions}\\n{query}'\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# see how langchain organizes the input to construct the result.\n",
    "# you can do so by printing the input of the chat_model.\n",
    "print_with_type(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the list of papers lacks substantial information and contains many inaccuracies. \n",
    "\n",
    "This is because the model has no knowlege about Prof. Wei Xu.  \n",
    "\n",
    "We will now demonstrate how to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adding more contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Allowing the model to search the web: Retrievers\n",
    "\n",
    "Many LLM applications require user-specific data that is not part of the model's training set, like the above example : )\n",
    "The primary way of accomplishing this is through **Retrieval Augmented Generation (RAG)**. In this process, external data is retrieved and then passed to the LLM when doing the generation step. `Retriever` is an interface that returns documents given an unstructured query, which is used to provide the related contents to LLMs\n",
    "\n",
    "LangChain provides all the building blocks for RAG applications - from simple to complex, including document loaders, text embedding models and web searches.  We will introduce these models in Lab 4.  Here, we introduce three very basic retrievers that does web search and some local file access.  \n",
    "\n",
    "- [web page](https://python.langchain.com/docs/how_to/document_loader_web/)\n",
    "- [load Json](https://python.langchain.com/docs/how_to/document_loader_json/)\n",
    "- [load PDF files](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SerpAPI is a widely used API to access search engine results. It allows you to scrape and parse search engine data, providing a way to retrieve up-to-date information from the web.\n",
    "\n",
    "In LangChain, SerpAPI can be integrated as a retriever to enhance the capabilities of language models by providing them with the latest information from the web. This is particularly useful for applications that require current data that is not part of the model's training set.\n",
    "\n",
    "By using SerpAPI with LangChain, you can perform web searches and feed the retrieved data into the language model to generate more accurate and contextually relevant responses.\n",
    "\n",
    "In the following sections, we will demonstrate how to set up and use SerpAPI within LangChain to perform web searches and integrate the results into your language model workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'title': \"Nvidia Stock Held Up Better Than Mag 7 Peers During Thursday's Rout—Watch These Key Levels\", 'link': 'https://www.investopedia.com/nvidia-stock-held-up-better-than-mag-7-peers-during-thursday-rout-watch-these-key-levels-11696055', 'source': 'Investopedia', 'date': '2 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d3774ddee1dcf3353f5bc2/images/1facaa3a5b60e24b43c2f81805cf6b5b3be5815be8f76320.jpeg'}, {'title': 'Nvidia Stock Is Cheaper Than These 17 Stocks. What to Consider Before Selling Shares.', 'link': 'https://www.barrons.com/articles/nvidia-stock-price-cheaper-buy-sell-e3eb88d7', 'source': \"Barron's\", 'date': '11 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d3774ddee1dcf3353f5bc2/images/1facaa3a5b60e24b4c7b0aaf8781d047769a910f60ccdbb5.jpeg'}, {'title': 'Fund manager sends blunt message on Nvidia stock before conference', 'link': 'https://www.thestreet.com/investing/stocks/veteran-fund-managers-blunt-take-on-nvidia-raises-eyebrows', 'source': 'TheStreet', 'date': '12 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d3774ddee1dcf3353f5bc2/images/1facaa3a5b60e24bc3f521adf0ee9f31107994e50c988a68.jpeg'}, {'title': 'GeForce RTX 5070 Takes The Number One Spot On Amazon, More NVIDIA GPUs Replace AMD In Best Selling GPUs List', 'link': 'https://wccftech.com/geforce-rtx-5070-takes-the-number-one-spot-on-amazon/', 'source': 'Wccftech', 'date': '10 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d3774ddee1dcf3353f5bc2/images/1facaa3a5b60e24bfae7f5d60d411131c3c58ec69e5546a1.jpeg'}, {'title': \"NVIDIA claims to have shipped twice as many RTX 50 cards as RTX 40 since launch, but it's an unfair comparison\", 'link': 'https://videocardz-com.webpkgcache.com/doc/-/s/videocardz.com/newz/nvidia-claims-to-have-shipped-twice-as-many-rtx-50-cards-as-rtx-40-since-launch-but-its-an-unfair-comparison', 'source': 'VideoCardz.com', 'date': '10 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d3774ddee1dcf3353f5bc2/images/1facaa3a5b60e24babd727336341ea3db8d3f4b7e073af98.jpeg'}, {'title': 'Nvidia’s next chips are named after Vera Rubin, astronomer who discovered dark matter', 'link': 'https://www.cnbc.com/2025/03/13/nvidia-to-detail-vera-rubin-chips-at-gtc-conference.html', 'source': 'CNBC', 'date': '7 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d3774ddee1dcf3353f5bc2/images/1facaa3a5b60e24b66fb3dbf7d1dd2bb5aaa3616b9d0b852.jpeg'}, {'title': \"Nvidia's Huang, critics were wrong about quantum computing: D-Wave CEO\", 'link': 'https://finance.yahoo.com/video/nvidias-huang-critics-were-wrong-170000181.html', 'source': 'Yahoo Finance', 'date': '7 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d3774ddee1dcf3353f5bc2/images/1facaa3a5b60e24b8138a12753d5821b3cc4677d8b5018de.jpeg', 'label': 'Video'}]\n"
     ]
    }
   ],
   "source": [
    "# Using the search API\n",
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "results = search.run(\"Nvidia\")\n",
    "print_with_type(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the search and LLM together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: Pioneering Accelerated Computing and AI', 'brief_desc': \"Founded in 1993, NVIDIA is a global leader in accelerated computing. The company's invention of the GPU in 1999 has significantly impacted the PC gaming market, computer graphics, and accelerated computing. NVIDIA's advancements have also played a crucial role in the development of modern AI and are driving industrial digitalization across various sectors.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "class News(BaseModel):\n",
    "    title: str = Field(description=\"title of the news\")\n",
    "    brief_desc: str = Field(description=\"brief descrption of the corresponding news\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "        \"context\": search.run,  # passing a retriever\n",
    "        \"query\": RunnablePassthrough()\n",
    "}\n",
    "websearch_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "\n",
    "res = websearch_chain.invoke(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Debugging and Logging\n",
    "\n",
    "#### 2.2.1 The debug mode\n",
    "\n",
    "LangChain provides a ``debug`` mode that allows you to see detailed information about the execution of your chains and agents. When debug mode is enabled, LangChain will print verbose output showing:\n",
    "\n",
    "1. The exact prompts being sent to the LLM\n",
    "2. The raw responses received from the LLM\n",
    "3. The execution flow of chains and agents\n",
    "4. Any intermediate steps and tool calls\n",
    "\n",
    "This is extremely useful for:\n",
    "- Troubleshooting unexpected outputs\n",
    "- Understanding how your chains are processing data\n",
    "- Optimizing prompts\n",
    "- Identifying errors in your chain's logic\n",
    "\n",
    "You can enable debug mode using `set_debug(True)` and disable it with `set_debug(False)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] [1.34s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] [1.35s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the user query based on the following context: \\nNVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"title\\\": {\\\"description\\\": \\\"title of the news\\\", \\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"brief_desc\\\": {\\\"description\\\": \\\"brief descrption of the corresponding news\\\", \\\"title\\\": \\\"Brief Desc\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"title\\\", \\\"brief_desc\\\"]}\\n```\\nQuery: tell me about the company nvidia, and write a brief summary for it\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [4.94s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA: The Engine of Global AI Infrastructure\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": []\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA: The Engine of Global AI Infrastructure\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\\\"\\n}\\n```\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 77,\n",
      "                \"prompt_tokens\": 294,\n",
      "                \"total_tokens\": 371\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": []\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-c43c24b7-42ec-4371-8089-b56cc2084b7b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 77,\n",
      "      \"prompt_tokens\": 294,\n",
      "      \"total_tokens\": 371\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [9ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA: The Engine of Global AI Infrastructure\",\n",
      "  \"brief_desc\": \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [6.31s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA: The Engine of Global AI Infrastructure\",\n",
      "  \"brief_desc\": \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: The Engine of Global AI Infrastructure', 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\"}\n"
     ]
    }
   ],
   "source": [
    "# Debugging and logging: debug mode\n",
    "from langchain.globals import set_debug\n",
    "set_debug(True)\n",
    "\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: The Engine of Global AI Infrastructure', 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"}\n"
     ]
    }
   ],
   "source": [
    "set_debug(False)\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\"tell me about the company nvidia, and write a brief summary for it\")\n",
    "\n",
    "print_with_type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Tracing with LangSmith\n",
    "\n",
    "LangSmith is a developer platform that helps you debug, test, evaluate, and monitor LLM applications. It provides:\n",
    "\n",
    "1. **Tracing**: Visualize the execution flow of your chains and agents\n",
    "2. **Debugging**: Inspect inputs, outputs, and intermediate steps\n",
    "3. **Evaluation**: Measure and compare model performance\n",
    "4. **Monitoring**: Track usage and performance in production\n",
    " \n",
    "To use LangSmith tracing:\n",
    "Set environment variables (in your .env file):\n",
    "   - `LANGCHAIN_TRACING_V2=\"true\"` to enable tracing\n",
    "   - `LANGCHAIN_API_KEY` with your LangSmith API key\n",
    "\n",
    "After running your code, you can view detailed traces at https://docs.smith.langchain.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'NVIDIA: The Engine of Global AI Infrastructure',\n",
       " 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\"}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrap_openai(openai.Client())\n",
    "\n",
    "@traceable # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    return websearch_chain.invoke(user_input)\n",
    "pipeline(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "# Out:  Hello there! How can I assist you today? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your task ####\n",
    "# go to the langsmith webpage and observe the traces. \n",
    "\n",
    "# one of my runs is shown in https://smith.langchain.com/public/df1f870f-badf-45a9-9f12-64824cf6a4b1/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': 'Multi-object events recognition from video sequences using extended finite state machine, Antiproliferative activities of the second-generation antipsychotic drug sertindole against breast'}\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# retrieve the information and fix the query results about Prof. Xu, \n",
    "# generating the correct Professor object.\n",
    "# Note that you do not have to get a perfect answer from the LLM in this lab.  \n",
    "# (if the answer is not perfect, please analyze and debug it in the next cell.)\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "# Define a new prompt to include the search results\n",
    "prompt_with_context = PromptTemplate(\n",
    "    template=\"Answer the user query based on the following context: \\nSearch Results: {context}\\n{format_instructions}\\nQuery: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "        \"context\": search.run,  # passing a retriever\n",
    "        \"query\": RunnablePassthrough()\n",
    "}\n",
    "# Create a new chain with the updated prompt\n",
    "professor_chain_with_context = setup_and_retrieval | prompt_with_context | chat_model | parser\n",
    "\n",
    "# Invoke the chain with the search results\n",
    "output_with_context = professor_chain_with_context.invoke(\"Provide the name and at least two publications of Professor Wei Xu of Tsinghua University. Note that he's a professor.\")\n",
    "\n",
    "print_with_type(output_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "# analyze the answer, if the answer is not correct, write down some comments \n",
    "# about why the answer is not correct. \n",
    "\n",
    "# One of the authors of the article \"Antiproliferative activities of the second-generation antipsychotic drug sertindole against breast\" is Wei Xu from the School of Meidicine, Tsinghua University. The LLM has no idea which Wei Xu is being referred to, therefore not providing the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Smarter workflow: Agents\n",
    "\n",
    "\n",
    "An AI agent is an autonomous system that perceives its environment, makes decisions, and takes actions to achieve specific goals. In the context of LLMs, an agent uses a language model as its reasoning engine to determine what actions to take and in what order, unlike chains where the sequence is predefined.\n",
    "\n",
    "LangChain provides several frameworks for building agents:\n",
    "1. **Tool integration**: LangChain allows agents to use external tools and APIs to gather information or perform actions\n",
    "2. **Agent types**: Supports various agent architectures like ReAct (Reasoning and Acting), Plan-and-Execute, and others\n",
    "3. **Memory systems**: Enables agents to maintain context across interactions (next lab)\n",
    "4. **Structured output**: Helps parse and validate agent responses\n",
    "\n",
    "For more advanced agent capabilities, LangGraph (an extension of LangChain) offers enhanced features for creating highly controllable and customizable agents with better state management and complex workflows (next lab).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Function-calling: Letter r's in straberry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following very simple example, and see if LLM can get it correct. (the correct answer is 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are two \\'r\\'s in the word \"strawberry.\"', response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 40, 'total_tokens': 55}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': []}}, id='run-4fb70469-7df0-494d-b870-afb6960f980a-0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"how many r's are there in the word strawberry?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the AI answered incorrectly by stating there are two 'r's in the word \"strawberry\" when there are actually three 'r's (st**r**awbe**rr**y). This demonstrates a limitation of LLMs in performing simple counting tasks. Even advanced models can make these basic errors because they process text holistically rather than character-by-character like humans do. This is why tools like function calling are useful - they allow us to delegate specific tasks (like counting) to dedicated functions that can perform them accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[StructuredTool(name='get_letter_count', description='get_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)', args_schema=<class 'pydantic.v1.main.get_letter_countSchema'>, func=<function get_letter_count at 0x7f7cbcd0c940>)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "# here is an example of a tool that can be used to count the number of a specific letter in a word.\n",
    "# note that we only use a single string parameter, because the simple agent only accept tools with a single parameter.\n",
    "# to fit two parameters, we need a clear format instruction for the input.\n",
    "@tool\n",
    "def get_letter_count(query: str) -> int:\n",
    "    \"\"\"Returns the number of a specific letter in a word. \n",
    "    The input should be in the format: word,letter (e.g., strawberry,r)\"\"\"\n",
    "    word, letter = query.split(',')\n",
    "    return word.count(letter)\n",
    "\n",
    "print(get_letter_count.invoke(\"strawberry,r\"))\n",
    "\n",
    "tools = [ get_letter_count ]\n",
    "\n",
    "print(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at counting letters in words.\", \n",
    "         ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count: get_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n",
      "Error in ConsoleCallbackHandler.on_tool_end callback: AttributeError(\"'int' object has no attribute 'strip'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [3.69s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I need to count the number of 'r' letters in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": []\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I need to count the number of 'r' letters in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 34,\n",
      "                \"prompt_tokens\": 205,\n",
      "                \"total_tokens\": 239\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": []\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-477bfcc0-0256-4a54-8962-e926b8859d14-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 34,\n",
      "      \"prompt_tokens\": 205,\n",
      "      \"total_tokens\": 239\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [3.70s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I need to count the number of 'r' letters in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] Entering Tool run with input:\n",
      "\u001b[0m\"strawberry,r\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"I need to count the number of 'r' letters in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count: get_letter_count(query: str) -> int - Returns the number of a specific letter in a word. \\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:I need to count the number of 'r' letters in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.86s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": []\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 23,\n",
      "                \"prompt_tokens\": 243,\n",
      "                \"total_tokens\": 266\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": []\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-7b383153-c612-473c-b7e2-3741cd5a7a72-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 23,\n",
      "      \"prompt_tokens\": 243,\n",
      "      \"total_tokens\": 266\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.86s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [5.58s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"There are 3 'r's in the word strawberry.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "\n",
    "agent_chain.invoke({\"input\": \"how many r's are there in the word strawberry?\"})\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers to sort: 83,22,35,53,1,23,39,72,52,85,78,95,69,6,63,94,26,46,2,11\n",
      "Sorted numbers: [1, 2, 6, 11, 22, 23, 26, 35, 39, 46, 52, 53, 63, 69, 72, 78, 83, 85, 94, 95]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'sort the numbers 83,22,35,53,1,23,39,72,52,85,78,95,69,6,63,94,26,46,2,11',\n",
       " 'output': 'The sorted numbers are 1, 2, 6, 11, 22, 23, 26, 35, 39, 46, 52, 53, 63, 69, 72, 78, 83, 85, 94, 95.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#### your task ####\n",
    "# implement a tool that sort an array of numbers (packed as a comma separated string)\n",
    "# then ask the agent to use the tool to sort an input array. \n",
    "@tool\n",
    "def sort_numbers(numbers: str) -> List[int]:\n",
    "    \"\"\"Sorts an array of numbers. The input should be a comma separated string of numbers.\"\"\"\n",
    "    return sorted([int(num) for num in numbers.split(',')])\n",
    "\n",
    "tools = [ sort_numbers ]\n",
    "    \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ( \"system\", \"You are very powerful assistant who can use tools, but bad at sorting numbers.\",\n",
    "         ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent_chain = initialize_agent(tools, \n",
    "                               chat_model, \n",
    "                               agent=\"zero-shot-react-description\", \n",
    "                               prompt_template=prompt, \n",
    "                               verbose=False\n",
    "                               )\n",
    "\n",
    "# Generate 20 random numbers between 1 and 100\n",
    "numbers_to_sort = \",\".join(map(str, random.sample(range(1, 101), 20)))\n",
    "print(f\"Numbers to sort: {numbers_to_sort}\")\n",
    "# Print the sorted numbers\n",
    "sorted_numbers = sort_numbers(numbers_to_sort)\n",
    "print(f\"Sorted numbers: {sorted_numbers}\")\n",
    "\n",
    "# Invoke the agent to sort the numbers\n",
    "agent_chain.invoke({\"input\": f\"sort the numbers {numbers_to_sort}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create an auto-web-search AI Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# read the following example about the built-in web-search tool and understand the code. \n",
    "\n",
    "\n",
    "from langchain.agents import load_tools \n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n",
    "        ),\n",
    "        (\"user\", \"{keyword}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [load_tools([\"serpapi\"], chat_model)[0]]\n",
    "\n",
    "agent_chain = initialize_agent(tools, chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI don't have direct access to real-time news or the ability to browse the internet for the latest information. However, I can suggest a way to find the most recent news from Tsinghua University.\n",
      "\n",
      "Final Answer: To get the latest news from Tsinghua University within the last week, I recommend visiting the official Tsinghua University website or checking their social media pages for the most up-to-date information. If you need help with specific information or have a particular topic in mind, feel free to let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me the news from tsinghua university within last week?',\n",
       " 'output': 'To get the latest news from Tsinghua University within the last week, I recommend visiting the official Tsinghua University website or checking their social media pages for the most up-to-date information. If you need help with specific information or have a particular topic in mind, feel free to let me know!'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"tell me the news from tsinghua university within last week?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find specific publications by Prof. Wei Xu from IIIS, Tsinghua University. The best way to find this information is through a web search.\n",
      "Action: Search\n",
      "Action Input: Prof. Wei Xu publications IIIS Tsinghua University\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['I am a professor at the Institute for Interdisciplinary Information Sciences of Tsinghua University in Beijing. I have a broad research interest in distributed ...', 'Wei Xu. Professor, IIIS and CollegeAI, Tsinghua University. Verified email at tsinghua.edu.cn - Homepage · Computer Science. ArticlesCited byPublic access ...', 'Wei Xu. Vice Dean, Professor Institute for Interdisciplinary Information Sciences, Tsinghua University. Office: FIT-4-6005, Tsinghua University, Beijing, China', 'Wei XU | Cited by 722 | of Tsinghua University, Beijing (TH) | Read 29 publications | Contact Wei XU.', \"Beijing Pioneer in Teacher's Ethics. 2018. Tsinghua University Lab Design Award (3rd class). 2018. Tsinghua University Excellent Performance Employee Award.\", 'This page describes the dataset and demo used in the following paper. Large-scale system problem detection by mining console logs. Wei Xu, Ling Huang, Armando ...', 'View the Special Programme for Research & Training in Tropical Diseases profile of Jian-wei Xu. Including their publications and grants.', 'Master in Computer Science, 2022-2025. Institute for Interdisciplinary Information Sciences, Tsinghua University Advisor: Prof. Wei Xu · Bachelor in Computer ...', 'Publications. Publication (1). FIG. 1. Device structure and working mechanism. (a) Schematic of the. FIG. 3. The first demonstration of an organic transistor ...', 'Worked as a postdoctoral fellow in Tsinghua University from 2016 to 2020. He joined Agricultural Genomics Institute at Shenzhen in 2020, as a principle ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results provide some information about Prof. Wei Xu's publications, but they are not explicitly listed. I need to refine the search to find specific titles of his publications.\n",
      "Action: Search\n",
      "Action Input: Prof. Wei Xu IIIS Tsinghua University publications list\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['(For a full list of publications, please see my Google Scholar Page.) Training Job Placement in Clusters with Statistical In-Network Aggregation. Bohan Zhao, ...', 'Wei Xu. Professor, IIIS and CollegeAI, Tsinghua University. Verified email at tsinghua.edu.cn - Homepage · Computer Science. ArticlesCited byPublic access ...', 'Wei Xu. Professor, IIIS and CollegeAI, Tsinghua University. 在 tsinghua.edu.cn 的电子邮件经过验证 - 首页 · Computer Science. 文章引用次数开放获取的出版物 ...', 'Tsinghua University. Assistant Professor at Institute for Interdisciplinary Information Sciences IIIS. Research Area: Distributed Systems + Machine Learning.', 'List of computer science publications by Wei Xu. ... [0000-0002-9271-0223] — Tsinghua University, Institute for Theoretical Computer Science, Beijing, China ...', \"Wei Xu's 6 research works with 114 citations, including: Multi-object events recognition from video sequences using extended finite state machine.\", 'Publications ; Jul 2024 · Kehan Tao ; Jun 2024 · Xin Liu ; May 2024 · Zhenzhen Wang ; Jun 2023 · Liying Wang ; May 2023 · Wei Xu ...', 'Wei Xu ; Applying HCAI in Developing Effective Human-AI Teaming: A Perspective from Human-AI Joint Cognitive Systems · Wei Xu. Zhejiang University ; From ...', 'Publications · On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena · The Impact of Visual Information in Chinese ...', 'Jun 2024 - Present President, IIIS Graduate Students Union, Tsinghua University. In charge of organization and publicity. May 2024 - Present Freshman Counselor ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results now provide a clearer list of publications by Prof. Wei Xu. I can extract at least two specific publications from the information provided.\n",
      "Thought: I now know the final answer\n",
      "Final Answer: Two of Prof. Wei Xu's publications from IIIS, Tsinghua University are:\n",
      "1. \"Training Job Placement in Clusters with Statistical In-Network Aggregation\" by Bohan Zhao, et al.\n",
      "2. \"On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'list explicitly at least two of the publications of Prof. Wei Xu in IIIS, Tsinghua University. Note that he is a professor, not a student.',\n",
       " 'output': 'Two of Prof. Wei Xu\\'s publications from IIIS, Tsinghua University are:\\n1. \"Training Job Placement in Clusters with Statistical In-Network Aggregation\" by Bohan Zhao, et al.\\n2. \"On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena\"'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# use the web search tool to find out about \n",
    "# prof. wei xu and his publication list.  \n",
    "# Compare the results with the previous implementation. is it better or worse?\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n",
    "        ),\n",
    "        (\"user\", \"{keyword}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), \n",
    "    ]\n",
    ")\n",
    "tools = [load_tools([\"serpapi\"], chat_model)[0]]\n",
    "agent_chain.invoke(\"list explicitly at least two of the publications of Prof. Wei Xu in IIIS, Tsinghua University. Note that he is a professor, not a student.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use one of the built-in tool for langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to calculate the definite integral of x^2 from 0 to 1. The integral of x^2 is (1/3)x^3, and I need to evaluate this from 0 to 1.\n",
      "Action: Calculator\n",
      "Action Input: (1/3)*1^3 - (1/3)*0^3\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 0.3333333333333333\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The integral of x^2 from 0 to 1 is 1/3 or approximately 0.333.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me the integral of x^2 from 0 to 1',\n",
       " 'output': 'The integral of x^2 from 0 to 1 is 1/3 or approximately 0.333.'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### your task ####\n",
    "# use a built-in tool in langchain to complete a task of your choice. \n",
    "# in the comment, please describe the task and the tool you used. \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users solve math problems: {keyword}\",\n",
    "        ),\n",
    "        (\"user\", \"{keyword}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # used to store the previous agent tool invocations and the corresponding tool outputs. \n",
    "    ]\n",
    ")\n",
    "\n",
    "tools = [load_tools([\"llm-math\"], chat_model)[0]]\n",
    "agent_chain = initialize_agent(tools, chat_model, agent=\"zero-shot-react-description\", prompt_template=prompt, verbose=True)\n",
    "agent_chain.invoke(\"tell me the integral of x^2 from 0 to 1\")\n",
    "# optional: try to use more than one tool in the same agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
